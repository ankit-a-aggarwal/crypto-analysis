{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- Make generic functions for tasks for modularity\n",
    "- CRON jobs for all timeframes\n",
    "- One function to update all csvs\n",
    "- Plug n play for indicators\n",
    "- Add limit to queries\n",
    "- Include Active/Inactive column in CSV for active/inactive coins\n",
    "- Update_CSV_to_Latest should contain active coins from Exchanges and From CSV. Check if a (coin,exchange) tuple is active(Check if it is present on exchange using ccxt library) . If it is active   get latest data for it if already present in CSV,if not in CSV get all data. If coin is not active on exchange , we will put a Active/Inactive status in CSV accordingly. All functions will have   to be modified to run code only for active coin-exchange combinations.\n",
    "- Date Format will be '%d-%m-%Y %H:%M:%S' . This is giving me a lot of problems especially while reading data. When I don't put :%S it tells me dataframe has second and sometimes when :%S is         there, it tells me no second value in dataframe.\n",
    "- Analytics Value Accuracy. Some parameter in Jupyter.\n",
    "- Have to fetch Coins based on Parameters. Example - Fetch all active coins-exchange combinations where RSI>0 and RSI<=30. Fetch all active coins-exchange combinations where closing price is         between LOWERBAND and MIDDLEBAND. Get me intersection(common coins) of these 2 list. Now the coin from the intersection list which probably has the lowest volume can increase in price faster       then the others(Little increase in Volume will result in Big increase in Price)\n",
    "- For each active coin-exchange combination I want to check the change in Value of different Technical Indicators of 2 consecutive periods in time. Example - I want to know if for a particular       coin RSI=a on period x and RSI>a on period x+1. I want to know whenever MACD and MACD_SIGNAL cross each other(On period x MACD=a and MACD_SIGNAL=b where a<=b and on period x+1 MACD=a and           MACD_SIGNAL=b where a>b. MACD_HISTOGRAM same like RSI want to know when it is 'a' on period x and 'a++' on period x+1).\n",
    "- Convert 1D timeframe to 3D/1Week/etc. Convert 1H timeframe to 4H/6H/etc.\n",
    "- Batch Processing of downloading new coins data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import talib\n",
    "import logging\n",
    "import requests\n",
    "import datetime\n",
    "import importlib\n",
    "import dateutil.parser\n",
    "import ccxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import inspect\n",
    "import pyti\n",
    "from apscheduler.schedulers.background import BackgroundScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(logging)\n",
    "LOGGING_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(format=LOGGING_FORMAT, level=logging.\n",
    "                    INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptocompare_wrapper = 'D:\\\\crypto_analysis-master-master\\\\crypto_analysis-master-master\\\\notebooks\\\\cryptocompare_wrapper.py'\n",
    "cryptocompare_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cryptocompare_wrapper as ccw\n",
    "reload(ccw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIELDS\n",
    "PRICE = 'PRICE'\n",
    "HIGH = 'HIGH24HOUR'\n",
    "LOW = 'LOW24HOUR'\n",
    "VOLUME = 'VOLUME24HOUR'\n",
    "CHANGE = 'CHANGE24HOUR'\n",
    "CHANGE_PERCENT = 'CHANGEPCT24HOUR'\n",
    "MARKETCAP = 'MKTCAP'\n",
    "NPERIODS = 100\n",
    "TIMEFRAME = 'Day'\n",
    "datetimeStringformat_to_csv = \"%d-%m-%Y %H:%M\"\n",
    "data_directory = 'D:\\\\crypto_analysis-master-master\\\\crypto_analysis-master-master\\\\data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults\n",
    "CURR = 'BTC'\n",
    "EXCHANGE = 'CCCAGG'\n",
    "COIN = 'ETH'\n",
    "COIN_LIST = ['BTC', 'ETH', 'XRP']\n",
    "EXCHANGES = ['Bittrex','Binance','Kucoin','HuobiPro','Cryptopia','IDEX']\n",
    "EXCHANGES = ['Binance']\n",
    "bittrex_exchange = ccxt.bittrex()\n",
    "binance_exchange = ccxt.binance()\n",
    "kucoin_exchange = ccxt.kucoin()\n",
    "huobiPro_exchange = ccxt.huobipro()\n",
    "cryptopia_exchange = ccxt.cryptopia()\n",
    "bitmex_exchange = ccxt.bitmex()\n",
    "#print(bittrex_exchange.fetchCurrencies())\n",
    "list_of_exchanges = [bittrex_exchange,binance_exchange,kucoin_exchange,huobiPro_exchange,\n",
    "                     cryptopia_exchange]\n",
    "#list_of_exchanges = [bitmex_exchange]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every exchange, fetch it's markets. Then depending on the JSON returned, prepare a list of coins for which historical data has to be downloaded.\n",
    "def setupExchanges(list_of_exchanges):\n",
    "    done = False\n",
    "    i=0\n",
    "    #df_markets = pd.DataFrame(markets)\n",
    "    #bittrex_market = bittrex_exchange.fetchMarkets()\n",
    "    #binance_market = binance_exchange.fetchMarkets()\n",
    "    #kucoin_market = kucoin_exchange.fetchMarkets()\n",
    "    #list_of_markets = [#bittrex_market,\n",
    "                       #binance_market\n",
    "     #                  kucoin_market #For kucoin the fetchMarkets function returns different dictionary keys\n",
    "      #                  ]\n",
    "\n",
    "    var_quote = \"\"\n",
    "    coin_exchange_combination = {}\n",
    "    for exchange in list_of_exchanges:\n",
    "        coins_list = set()\n",
    "        #if exchange.name == 'Cryptopia' or exchange.name == 'Bittrex' or exchange.name == 'Kucoin' or exchange.name == 'Huobi Pro':\n",
    "            #continue #exchange.name == 'Binance' or \n",
    "        markets = exchange.fetchMarkets()\n",
    "        for row in markets:\n",
    "            if exchange.name == 'Huobi Pro' or exchange.name == 'Cryptopia':\n",
    "                if row['base'] not in coins_list:\n",
    "                        coins_list.add(row['base'])\n",
    "                #continue\n",
    "            if  'active' in row and row['active'] == True :\n",
    "                #print(exchange.name,row)\n",
    "                #sys.exit(\"Te\")\n",
    "                if exchange.name == 'Bittrex' or exchange.name == 'Binance'  :\n",
    "                    var_quote = \"quoteId\"\n",
    "                elif   exchange.name == 'Kucoin' or exchange.name == 'Huobi Pro':\n",
    "                    var_quote = \"quote\"\n",
    "                #print(var_quote)\n",
    "                if var_quote in row and row[var_quote] == 'BTC':\n",
    "                    if row['base'] not in coins_list:\n",
    "                        coins_list.add(row['base'])\n",
    "        coin_exchange_combination[exchange.name] = coins_list\n",
    "    return coin_exchange_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(timestamp):\n",
    "    \"\"\" Convert timestamp into readable datetime \"\"\"\n",
    "    try:\n",
    "        return datetime.datetime.fromtimestamp(int(timestamp)).strftime('%d-%m-%Y %H:%M')\n",
    "        #return timestamp\n",
    "    except Exception as e:\n",
    "        logging.debug(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_and_delete_coin_exchange_combination(csv_filename_read,csv_filename_write,timeframe,coin_exchange_combination):\n",
    "    #For every exchange, download the new coins and delete coins which are delisted from that exchange\n",
    "    csv_all_coins_full = csv_filename_read\n",
    "    csv_all_coins_full_new = csv_filename_write\n",
    "    not_updated = defaultdict(list)\n",
    "    existing_coin_exchange = []\n",
    "    # If the csv already exists, find out which coins and exchanges have already been added\n",
    "    if os.path.isfile(csv_all_coins_full):\n",
    "        df_csv_all_coins_full = pd.read_csv(csv_all_coins_full,index_col=['exchange','coin'])\n",
    "        # existing_coin_exchange is a list of tuples (coin, exchange)\n",
    "        existing_coin_exchange = np.unique(df_csv_all_coins_full.index.values)\n",
    "        coin_exchange_combination_in_excel = {}\n",
    "        for a, b in existing_coin_exchange:\n",
    "            coin_exchange_combination_in_excel.setdefault(a, []).append(b)\n",
    "    number_of_coins = 0\n",
    "    coin_exchange_combination_to_delete = {}\n",
    "    for exchange in coin_exchange_combination :\n",
    "        if exchange not in coin_exchange_combination_in_excel:\n",
    "            continue\n",
    "        coins_list_from_exchange = coin_exchange_combination[exchange]\n",
    "        coins_list_from_excel = coin_exchange_combination_in_excel[exchange]\n",
    "        coins_to_download = list(set(coins_list_from_exchange) - set(coins_list_from_excel))\n",
    "        coins_to_delete = list(set(coins_list_from_excel) - set(coins_list_from_exchange))\n",
    "        coin_exchange_combination_to_delete[exchange] = coins_to_delete\n",
    "        print(exchange,coins_to_download)\n",
    "        for symbol in coins_to_download:\n",
    "         #For every symbol-exchange combination, if it is present in CSV,don't download historical Data for it.\n",
    "            try:\n",
    "                # Can't fetch the same symbol in same symbol rate\n",
    "                print(symbol,exchange,\"In Try Block\")\n",
    "                func = function_period_mapping[timeframe]\n",
    "                to_curr = 'BTC'\n",
    "                if symbol == \"BTC\":\n",
    "                    to_curr = \"USD\"\n",
    "                if exchange == 'IDEX':\n",
    "                    to_curr = 'ETH'\n",
    "                if symbol is not to_curr:\n",
    "                    df_coin_all = func(\n",
    "                        coin=symbol,\n",
    "                        to_curr=to_curr,\n",
    "                        timestamp=time.time(),\n",
    "                        exchange=exchange\n",
    "                    )\n",
    "    \n",
    "                if df_coin_all.empty:\n",
    "                    not_updated[exchange].append(symbol)\n",
    "                    #print(symbol,exchange)\n",
    "                else:\n",
    "                    df_coin_all['exchange'] = exchange\n",
    "                    df_coin_all['coin'] = symbol\n",
    "                    print(\"Coin Inserted\")\n",
    "                    df_coin_all = df_coin_all.reset_index()\n",
    "                    df_coin_all['time'] = df_coin_all.unix_timestamp.apply(convert_timestamp)\n",
    "                    print(\"Time column created\")\n",
    "                    df_coin_all = df_coin_all.set_index(['coin','exchange', 'unix_timestamp'])\n",
    "                    print(\"Index Set Again\")\n",
    "                    # If csv does not exist, write, else append\n",
    "                    if not os.path.isfile(csv_all_coins_full_new):\n",
    "                        print(\"New File Created\")\n",
    "                        df_coin_all.to_csv(csv_all_coins_full_new, mode='w')\n",
    "                    else:\n",
    "                        print(\"Appended\")\n",
    "                        df_coin_all.to_csv(csv_all_coins_full_new, mode='a', header=False)\n",
    "                    number_of_coins = number_of_coins +1\n",
    "    \n",
    "            except Exception as e:\n",
    "                logging.error(e)\n",
    "                # logging.debug(\"Could not update data for {curr} from {exchange}\".format(curr=symbol, exchange=exchange))\n",
    "                not_updated[exchange].append(symbol)\n",
    "    \n",
    "    logging.error(\"Did not update the following. Try again.\\n {not_updated}\".format(not_updated=not_updated))\n",
    "    print(number_of_coins)\n",
    "    print(coin_exchange_combination_to_delete)\n",
    "    delete_coins_from_CSV(coin_exchange_combination_to_delete,df_csv_all_coins_full.reset_index(),csv_filename_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_coins_from_CSV(coin_exchange_combination_to_delete,df_csv_all_coins_full,csv_filename_read):\n",
    "    for exchange in coin_exchange_combination_to_delete:\n",
    "        coins_list_to_delete = coin_exchange_combination_to_delete[exchange]\n",
    "        for symbol in coins_list_to_delete:\n",
    "            df_csv_all_coins_full = df_csv_all_coins_full[~((df_csv_all_coins_full['coin'] == symbol) & (df_csv_all_coins_full['exchange'] == exchange))] \n",
    "    df_csv_all_coins_full.set_index(['coin', 'exchange','unix_timestamp']).to_csv(csv_filename_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins_list_from_exchange = setupExchanges(list_of_exchanges)\n",
    "update_and_delete_coin_exchange_combination(data_directory+'all_coins_day_full_1day.csv',data_directory+'all_coins_day_full_1day_new_coins.csv','1dayfull',coins_list_from_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coins_list = [\"BTC\"]\n",
    "coins_list = [\"\"]\n",
    "setupExchanges(list_of_exchanges)\n",
    "update_and_delete_coin_exchange_combination(data_directory+'all_coins_hour_full_1hour_.csv',data_directory+'all_coins_hour_full_1hour_.csv','1hour',coins_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps csv (future data objects) to period granularity\n",
    "# If we store all data together in a single data source, we'll change this to a function which returns corresponding rows\n",
    "data_csv_period_mapping = {\n",
    "    \"1day\": data_directory+'all_coins_day_full_1day.csv',\n",
    "    \"1hour\": data_directory+'all_coins_hour_full_1hour.csv',\n",
    "    \"1min\": data_directory+'all_coins_min_full_1min.csv',\n",
    "    \"1daycryptopia\":data_directory+'all_coins_day_full_1day_Cryptopia.csv',\n",
    "    '1daybtcbitfinex':data_directory+'BTC_Bitfinex_day_full_1day.csv',\n",
    "    '1hourbtcbitfinex':data_directory+'BTC_Bitfinex_hour_full_1hour.csv'\n",
    "}\n",
    "frequency_resampling_period_mapping = {\n",
    "    \"day\":'D',\n",
    "    \"hour\":'H',\n",
    "    \"min\":'M'\n",
    "}\n",
    "function_period_mapping = {\n",
    "    '1day': ccw.get_historical_price_day,\n",
    "    '1hour': ccw.get_historical_price_hour,\n",
    "    '1min': ccw.get_historical_price_minute,\n",
    "    '1dayfull' : ccw.get_historical_price_day_full,\n",
    "    '1daycryptopia' : ccw.get_historical_price_day,\n",
    "    '1daybtcbitfinex':ccw.get_historical_price_day,\n",
    "    '1hourbtcbitfinex':ccw.get_historical_price_hour\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_csv = pd.read_csv('all_coins_day_full.csv', index_col=None)\n",
    "indicator_list = ['unix_timestamp','BBANDS_BANDWIDTH_PERCENT','MONEY_FLOW_INDEX',\n",
    "                   'STOCH_PERCENT_K_MONEY_FLOW_INDEX','STOCH_PERCENT_D_MONEY_FLOW_INDEX','RSI','RSI_OVER_BOUGHT','RSI_OVER_SOLD',\n",
    "                   'STOCHRSI_K','STOCHRSI_D','STOCH_PERCENT_K','STOCH_PERCENT_D','STOCH_OVER_BOUGHT','STOCH_OVER_SOLD','SMA_FAST','SMA_SLOW','SMA_TEST',\n",
    "                  'MACD','MACD_SIGNAL','MACD_TEST','ON_BALANCE_VOLUME','ON_BALANCE_VOLUME_TEST']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Technical Analysis Settings\n",
    "EMA_FAST = 10\n",
    "EMA_SLOW = 20\n",
    "RSI_PERIOD = 14\n",
    "RSI_OVER_BOUGHT = 70\n",
    "RSI_OVER_SOLD = 30\n",
    "RSI_AVG_PERIOD = 15\n",
    "MACD_FAST = 12\n",
    "MACD_SLOW = 26\n",
    "MACD_SIGNAL = 9\n",
    "STOCH_K = 14\n",
    "STOCH_D = 3\n",
    "STOCH_OVER_BOUGHT = 70\n",
    "STOCH_OVER_SOLD = 30\n",
    "from pyti import bollinger_bands\n",
    "from pyti import money_flow_index\n",
    "from pyti import stochastic\n",
    "from pyti import simple_moving_average\n",
    "from pyti import stochrsi\n",
    "from pyti import on_balance_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_indicator(csv_filename,periods,timeframe,datetimeformat_string):\n",
    "    \"\"\" Update the given csv_file with new column values for corr rows \"\"\"\n",
    "    df_csv = pd.read_csv(csv_filename, index_col=None,dayfirst=True)\n",
    "    df_csv.drop_duplicates(subset=['coin','exchange','unix_timestamp'],inplace=True)\n",
    "    #df_csv.time = df_csv.time.apply(lambda t: datetime.datetime.strptime(t, '%d-%m-%Y %H:%M:%S'))\n",
    "    \n",
    "    for indicator in indicator_list:\n",
    "        if indicator not in df_csv.columns and indicator not in df_csv.index:\n",
    "            df_csv[indicator] = np.nan\n",
    "    #df_csv.unix_timestamp =  df_csv.time.apply(lambda t: time.mktime(datetime.datetime.strptime(str(t) ,'%d-%m-%Y %H:%M').timetuple()))\n",
    "    df_csv = df_csv.set_index(['coin', 'exchange','unix_timestamp'])\n",
    "    data = list(df_csv.index.get_level_values(0).unique())\n",
    "    i=0\n",
    "    j=0\n",
    "    for coin_name in data:\n",
    "        coin_df = df_csv[df_csv.index.get_level_values(0)==coin_name] \n",
    "        #print(coin_df)\n",
    "        coin_df = coin_df.reset_index()\n",
    "        coin_df = coin_df.sort_values(by=['exchange','unix_timestamp']).set_index(['coin', 'exchange','unix_timestamp'])\n",
    "        #print(coin_df)\n",
    "        df_groupby = coin_df.groupby(['exchange'], group_keys=False)\n",
    "        for key, item in df_groupby:\n",
    "            req_data = df_groupby.get_group(key)\n",
    "            req_data2 = req_data.iloc[-periods:]\n",
    "\n",
    "            start_date = req_data2.index.get_level_values(2)[0]\n",
    "            end_date = req_data2.index.get_level_values(2)[req_data2.shape[0]-1]\n",
    "            req_data2 = req_data[(req_data.index.get_level_values(2) >= start_date) & (req_data.index.get_level_values(2) <= end_date)]\n",
    "            #print(req_data2)\n",
    "            np_volumeto = np.array(req_data2.volumeto.values,dtype='f8')\n",
    "            if len(np_volumeto)<20:\n",
    "                j = j+1\n",
    "                print(coin_name,j,\" Not Updated\")\n",
    "                continue\n",
    "            req_data2['BBANDS_BANDWIDTH_PERCENT'] = pyti.bollinger_bands.percent_b(req_data2.close.values,20)\n",
    "            req_data2['MONEY_FLOW_INDEX'] = money_flow_index.money_flow_index(req_data2.close.values, req_data2.high.values, req_data2.low.values, np_volumeto, 14)\n",
    "            req_data2['STOCH_PERCENT_K_MONEY_FLOW_INDEX'] = pyti.stochastic.percent_k(req_data2.MONEY_FLOW_INDEX.values,14) * 100\n",
    "            req_data2['STOCH_PERCENT_D_MONEY_FLOW_INDEX'] = pyti.simple_moving_average.simple_moving_average(req_data2.STOCH_PERCENT_K_MONEY_FLOW_INDEX.values,3)\n",
    "            req_data2['RSI'] = talib.RSI(req_data2.close.values, timeperiod=RSI_PERIOD)\n",
    "            req_data2['RSI_OVER_BOUGHT'] = np.where((req_data2.RSI >= RSI_OVER_BOUGHT) & (req_data2.RSI <= req_data2.RSI.shift(1)),1,0)\n",
    "            req_data2['RSI_OVER_SOLD'] = np.where((req_data2.RSI <= RSI_OVER_SOLD) & (req_data2.RSI >= req_data2.RSI.shift(1)),1,0)\n",
    "            req_data2['STOCHRSI_K'] = pyti.stochrsi.stochrsi(req_data2.close.values,14)\n",
    "            req_data2['STOCHRSI_D'] = pyti.simple_moving_average.simple_moving_average(req_data2.STOCHRSI_K.values,3)\n",
    "            req_data2['STOCH_PERCENT_K'] = pyti.stochastic.percent_k(req_data2.high.values,14) * 100\n",
    "            req_data2['STOCH_PERCENT_D'] = pyti.simple_moving_average.simple_moving_average(req_data2.STOCH_PERCENT_K.values,3)\n",
    "            req_data2['STOCH_OVER_BOUGHT'] = np.where((req_data2.STOCH_PERCENT_K >= STOCH_OVER_BOUGHT) & (req_data2.STOCH_PERCENT_K <= req_data2.STOCH_PERCENT_K.shift(1)),1,0)\n",
    "            req_data2['STOCH_OVER_SOLD'] = np.where((req_data2.STOCH_PERCENT_K <= STOCH_OVER_SOLD) & (req_data2.STOCH_PERCENT_K >= req_data2.STOCH_PERCENT_K.shift(1)),1,0)\n",
    "            req_data2['SMA_FAST'] = talib.SMA(req_data2.close.values,7)\n",
    "            req_data2['SMA_SLOW'] = talib.SMA(req_data2.close.values,21)\n",
    "            req_data2['SMA_TEST'] = np.where(req_data2.SMA_FAST>req_data2.SMA_SLOW,1,0)\n",
    "            req_data2['ON_BALANCE_VOLUME'] = on_balance_volume.on_balance_volume(req_data2.close.values,np_volumeto)\n",
    "            req_data2['ON_BALANCE_VOLUME_TEST'] = np.where(req_data2.ON_BALANCE_VOLUME>req_data2.ON_BALANCE_VOLUME.shift(1),1,0)\n",
    "            \"\"\"\n",
    "            req_data2['Accumulation_Distribution_Oscillator'] = talib.ADOSC(req_data2.high.values,req_data2.low.values\n",
    "                                                  ,req_data2.close.values,np_volumeto)\n",
    "            req_data2['ADOSC_TEST'] = np.where((req_data2.Accumulation_Distribution_Oscillator>req_data2.Accumulation_Distribution_Oscillator.shift(1)) & (req_data2.Accumulation_Distribution_Oscillator>=0) & \n",
    "                                               (req_data2.Accumulation_Distribution_Oscillator.shift(1)<=0),1,0)\n",
    "            \"\"\"\n",
    "            \n",
    "            req_data2['MACD'],req_data2['MACD_SIGNAL'],MACD_HISTOGRAM= talib.MACD(req_data2.close.values,fastperiod=MACD_FAST,slowperiod=MACD_SLOW,signalperiod=MACD_SIGNAL)\n",
    "            req_data2['MACD_TEST'] = np.where(req_data2.MACD>req_data2.MACD_SIGNAL,1,0)\n",
    "            \n",
    "            \n",
    "            df_csv.update(req_data2)\n",
    "            i = i+1\n",
    "            print(coin_name,i)\n",
    "            #print(df_csv.query('coin == @coin_name').tail(1))\n",
    "            #sys.exit(\"Testing\")\n",
    "    df_csv.to_csv(csv_filename,date_format=datetimeStringformat_to_csv)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator(data_directory+'all_coins_day_full_1day.csv',250,'1day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator(data_directory+'all_coins_day_full_3days.csv',250,'3day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator(data_directory+'all_coins_day_full_7days.csv',250,'7day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator(data_directory+'all_coins_day_full_10days.csv',250,'10day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator(data_directory+'all_coins_day_full_14days.csv',250,'14day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator(data_directory+'all_coins_day_full_1day_Cryptopia.csv',250,'1day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator(data_directory+'all_coins_day_full_3days_Cryptopia.csv',250,'3day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator(data_directory+'all_coins_day_full_7days_Cryptopia.csv',250,'7day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator(data_directory+'all_coins_day_full_14days_Cryptopia.csv',250,'14day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator(data_directory+'all_coins_min_full_1min.csv',250,'1min')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(csv_filename,period,resampling_multiplier,exchange,datetimeformat_string,output_file_name):\n",
    "    df_csv = pd.read_csv(csv_filename,dayfirst=True)\n",
    "    #df_csv['time'] = df_csv.unix_timestamp.apply(lambda t: datetime.datetime.fromtimestamp(int(t)).strftime(datetimeformat_string))\n",
    "    df_csv.unix_timestamp = pd.to_datetime(df_csv.unix_timestamp,unit=\"s\",utc=True)\n",
    "    #df_csv['time'].to_csv('time.csv')\n",
    "    df_csv = df_csv.reset_index()\n",
    "    #print(df_csv.columns)\n",
    "    for indicator in indicator_list:\n",
    "        if indicator not in df_csv.columns:\n",
    "            df_csv[indicator] = np.nan\n",
    "    df_csv = df_csv.set_index(['coin', 'exchange','unix_timestamp'])\n",
    "    data = list(df_csv.index.get_level_values(0).unique())\n",
    "    i=0\n",
    "    all_dataframes = []\n",
    "    resampling_period = \"\"+str(resampling_multiplier)+frequency_resampling_period_mapping[period]\n",
    "    output_csv_filename = output_file_name\n",
    "    for coin_name in data:\n",
    "        coin_df = df_csv[df_csv.index.get_level_values(0)==coin_name] \n",
    "        coin_df = coin_df.reset_index()\n",
    "        coin_df = coin_df.sort_values(by=['exchange','unix_timestamp']).set_index(['coin', 'exchange','unix_timestamp'])\n",
    "        #print(coin_df)\n",
    "        df_groupby = coin_df.groupby(['exchange'], group_keys=False)\n",
    "        for key, item in df_groupby:\n",
    "            req_data = df_groupby.get_group(key)\n",
    "            #print(type(req_data.index))\n",
    "            req_data = req_data.resample(resampling_period,level=2,closed='right',label='right').agg({'open': 'first', \n",
    "                                     'high': 'max', \n",
    "                                     'low': 'min', \n",
    "                                     'close': 'last',\n",
    "                                    'volumeto':'sum',\n",
    "                                        'volumefrom':'sum'})\n",
    "\n",
    "            req_data['coin'] = coin_name\n",
    "            req_data['exchange'] = key\n",
    "            \n",
    "            #print(req_data)\n",
    "            req_data = req_data.reset_index()\n",
    "            req_data['unix_timestamp'] =  req_data.unix_timestamp.apply(lambda t: time.mktime(datetime.datetime.strptime(str(t), '%Y-%m-%d %H:%M:%S').timetuple()))\n",
    "            #req_data['time'].to_csv(\"Req_data_Time.csv\")\n",
    "            req_data['time'] = pd.to_datetime(req_data.unix_timestamp,unit=\"s\",utc=True)\n",
    "\n",
    "            req_data = req_data.set_index(['coin','exchange','unix_timestamp'])\n",
    "            i = i+1\n",
    "            print(coin_name,i)\n",
    "            #req_data = req_data.drop(labels=['time'],axis=1)\n",
    "            all_dataframes.append(req_data)\n",
    "    dataframe_answer = pd.concat(all_dataframes)\n",
    "    #dataframe_answer['time'].to_csv('Time.csv')\n",
    "    dataframe_answer.to_csv(output_csv_filename,date_format=datetimeStringformat_to_csv)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1day'],'day',3,\"\",datetimeStringformat_to_csv,data_directory+'all_coins_day_full_3days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1day'],'day',7,\"\",datetimeStringformat_to_csv,data_directory+'all_coins_day_full_7days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1day'],'day',10,\"\",datetimeStringformat_to_csv,data_directory+'all_coins_day_full_10days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1day'],'day',14,\"\",datetimeStringformat_to_csv,data_directory+'all_coins_day_full_14days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1daycryptopia'],'day',3,\"Cryptopia\",datetimeStringformat_to_csv,data_directory+'all_coins_day_full_3days_Cryptopia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1daycryptopia'],'day',7,\"Cryptopia\",datetimeStringformat_to_csv,data_directory+'all_coins_day_full_7days_Cryptopia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1daycryptopia'],'day',14,\"Cryptopia\",datetimeStringformat_to_csv,data_directory+'all_coins_day_full_14days_Cryptopia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1hourbtcbitfinex'],'hour',4,\"Bitfinex\",datetimeStringformat_to_csv,data_directory+'BTC_Bitfinex_hour_full_4hours.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1hourbtcbitfinex'],'hour',6,\"Bitfinex\",datetimeStringformat_to_csv,data_directory+'BTC_Bitfinex_hour_full_6hours.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1hourbtcbitfinex'],'hour',12,\"Bitfinex\",datetimeStringformat_to_csv,data_directory+'BTC_Bitfinex_hour_full_12hours.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1daybtcbitfinex'],'day',3,\"Bitfinex\",datetimeStringformat_to_csv,data_directory+'BTC_Bitfinex_day_full_3days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1daybtcbitfinex'],'day',7,\"Bitfinex\",datetimeStringformat_to_csv,data_directory+'BTC_Bitfinex_day_full_7days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1daybtcbitfinex'],'day',14,\"Bitfinex\",datetimeStringformat_to_csv,data_directory+'BTC_Bitfinex_day_full_14days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_api(coin=COIN, to_curr=CURR, nperiods=1, period='1day',exchange_name=EXCHANGES[0]):\n",
    "    \"\"\" Fetch data for coin over nperiods\n",
    "        e.g. Get data for 'BTC' for past 12 hours in hours granularity\n",
    "    \"\"\"\n",
    "    period = period.lower()\n",
    "    func = function_period_mapping[period]\n",
    "    if exchange_name == 'IDEX':\n",
    "        to_curr = 'ETH'\n",
    "    if coin == 'BTC':\n",
    "        to_curr = 'USD'\n",
    "    coin_last_nperiods = func(\n",
    "        coin=coin,\n",
    "        to_curr=to_curr,\n",
    "        limit=nperiods,\n",
    "        exchange=exchange_name\n",
    "    )\n",
    "    #print(coin_last_nperiods.index.get_level_values(0))\n",
    "    #answer = coin_last_nperiods.iloc[-int(nperiods):]\n",
    "    #print(answer.index.get_level_values(0))\n",
    "    if coin_last_nperiods is not None:\n",
    "        return coin_last_nperiods.iloc[-int(nperiods):]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_latest_period_data(csv_filename,datetimeformat_string):\n",
    "    df_csv = pd.read_csv(csv_filename, index_col=None,dayfirst=True)\n",
    "    #df_csv.time = df_csv.time.apply(lambda t: datetime.datetime.strptime(t, '%d-%m-%Y %H:%M'))\n",
    "    #df_csv.unix_timestamp =  df_csv.time.apply(lambda t: time.mktime(datetime.datetime.strptime(str(t) ,'%d-%m-%Y %H:%M').timetuple()))\n",
    "    df_csv = df_csv.set_index(['coin', 'exchange','unix_timestamp'])\n",
    "    data = list(df_csv.index.get_level_values(0).unique())\n",
    "    i=0\n",
    "    j=0\n",
    "    final_dataframe = []\n",
    "    for coin_name in data:\n",
    "        coin_df = df_csv[df_csv.index.get_level_values(0)==coin_name] \n",
    "        #print(coin_df)\n",
    "        coin_df = coin_df.reset_index()\n",
    "        coin_df = coin_df.sort_values(by=['exchange','unix_timestamp']).set_index(['coin', 'exchange','unix_timestamp'])\n",
    "        #print(coin_df)\n",
    "        df_groupby = coin_df.groupby(['exchange'], group_keys=False)\n",
    "        for key, item in df_groupby:\n",
    "            req_data = df_groupby.get_group(key)\n",
    "            req_data = req_data[:-1]\n",
    "            #print(req_data.tail(1))\n",
    "            final_dataframe.append(req_data)\n",
    "    answer = pd.concat(final_dataframe).reset_index()\n",
    "    #answer.to_csv('answer.csv')\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_csv_to_latest(period='1day',datetimeformat_string='%d-%m-%Y %H:%M',final_csv_column_order=[]):\n",
    "    \"\"\" Update the csv for given period upto current time for coin \"\"\"\n",
    "    period = period.lower()\n",
    "    csv_filename = data_csv_period_mapping[period]  # Get corr csv\n",
    "    #csv_filename = 'Experiment.csv'\n",
    "    #df_coin_period = pd.read_csv(csv_filename)  # , index_col=['coin', 'exchange']\n",
    "    df_coin_period = delete_latest_period_data(csv_filename,datetimeStringformat_to_csv)\n",
    "    #print(df_coin_period.tail(1)['time'])\n",
    "    print(\"Updated Dataframe\")\n",
    "    csv_column_order = df_coin_period.columns.tolist()\n",
    "    df_coin_period = df_coin_period.set_index(keys=['coin', 'exchange'])\n",
    "    #df_coin_period.time = df_coin_period.time.apply(lambda t: datetime.datetime.strptime(t, '%d-%m-%Y %H:%M'))\n",
    "    \n",
    "    lst_new_data = []\n",
    "    PRINT_MSG = \"{:15} {!s:20} {!s:>20} {:>10}\"\n",
    "    logging.info(PRINT_MSG.format(\"Exchange\", \"Last Updated Time\", \"Elapsed Time\", \"nPeriodsAgo\"))\n",
    "    coins_in_csv = list(df_coin_period.index.get_level_values(0).unique())\n",
    "    #coins_in_csv = ['ZRX']\n",
    "    i=0\n",
    "    j=0\n",
    "    for coin in coins_in_csv:\n",
    "        df_coin_period_coin = df_coin_period.loc[coin]\n",
    "        # Group by exchange, sort on timestamp, and get the last row of that particular coin\n",
    "        last_update = df_coin_period_coin.groupby('exchange', group_keys=False).apply(lambda c: c.sort_values(by='unix_timestamp').tail(1))\n",
    "        logging.info(\"-\" * 10 + \" For coin - {}\".format(coin))\n",
    "        \n",
    "        for exchange in last_update.index.values: #For every coin exchange combination\n",
    "            last_updated_time = int(last_update.loc[exchange]['unix_timestamp']) #Get the time of the last row\n",
    "            #print(type(last_updated_time))\n",
    "            try:\n",
    "                # elapsed_time = datetime.datetime.now() - datetime.datetime.strptime(last_updated_time, '%Y-%m-%d %H:%M:%S')\n",
    "                # elapsed_time = datetime.datetime.now() - datetime.datetime.strptime(last_updated_time, '%d-%m-%Y %H:%M')\n",
    "                elapsed_time = int(time.time()) - last_updated_time\n",
    "            except ValueError as e:\n",
    "                logging.info(\"Failed to parse time {} for {}--{}\".format(last_updated_time, coin, exchange))\n",
    "                elapsed_time = datetime.datetime.now() - dateutil.parser.parse(last_updated_time)\n",
    "            nperiods_ago = 0\n",
    "            if period == '1day' or period == '1daycryptopia' or period =='1daybtcbitfinex':\n",
    "                nperiods_ago = elapsed_time/(60*60*24)\n",
    "            elif period == '1hour' or period == '1hourbtcbitfinex':\n",
    "                 nperiods_ago = elapsed_time/(60*60)\n",
    "            \"\"\"\n",
    "            nperiods_ago = elapsed_time / datetime.timedelta(days=1 if period == '1day' or period == '1daycryptopia' or period =='1daybtcbitfinex' else 0,\n",
    "                                                             hours=1 if period == '1hour' or period == '1hourbtcbitfinex' else 0,\n",
    "                                                             minutes=1 if period == '1min' else 0,\n",
    "                                                             seconds=1)\n",
    "            \"\"\"\n",
    "            nperiods_ago = np.floor(nperiods_ago)\n",
    "            #print(nperiods_ago)\n",
    "            logging.info(PRINT_MSG.format(exchange, last_updated_time, elapsed_time, nperiods_ago))\n",
    "            #if coin == 'BTC':\n",
    "              #  nperiods_ago = nperiods_ago-1\n",
    "            if nperiods_ago > 0:\n",
    "                \n",
    "                logging.info(\"Updating data for {coin}-{exchange} from {last_updated_time}\".format(\n",
    "                    coin=coin, exchange=exchange, last_updated_time=last_updated_time)\n",
    "                )\n",
    "                #sys.exit(\"Testing\")\n",
    "                new_data_coin_period = fetch_data_api(\n",
    "                    coin=coin,\n",
    "                    nperiods=nperiods_ago,\n",
    "                    period=period,\n",
    "                    exchange_name=exchange\n",
    "                )\n",
    "                #print(new_data_coin_period.shape)\n",
    "                if new_data_coin_period is None:\n",
    "                    print(coin,exchange,\" Info Not available from API\",str(j))\n",
    "                    j =j+1\n",
    "                    continue\n",
    "                new_data_coin_period['coin'] = coin\n",
    "                new_data_coin_period['exchange'] = exchange\n",
    "                new_data_coin_period = new_data_coin_period.reset_index()\n",
    "                new_data_coin_period['time'] =  pd.to_datetime(new_data_coin_period.unix_timestamp,unit=\"s\",utc=True)\n",
    "                i = i + 1\n",
    "                #print(coin,exchange,i)\n",
    "                lst_new_data.append(new_data_coin_period)\n",
    "    \n",
    "    #print(\"CSV column order \"+str(csv_column_order))\n",
    "    if lst_new_data:\n",
    "        df_new_data = pd.concat(lst_new_data)\n",
    "        df_new_data = df_new_data.reset_index()\n",
    "        curr_columns = df_new_data.columns.tolist()\n",
    "        #print(\"Current columns \"+str(curr_columns))\n",
    "        df_coin_period = df_coin_period.reset_index()\n",
    "        csv_column_order = df_coin_period.columns.tolist()\n",
    "        column_order = [col for col in csv_column_order if col in curr_columns]\n",
    "        #print(\"New Column Order \"+str(column_order))\n",
    "        df_new_data = df_new_data.reindex(columns=column_order)\n",
    "        df_coin_period=df_coin_period.append(df_new_data)\n",
    "        #df_new_data.to_csv(csv_filename, mode='a', header=False,index=False,date_format=\"%d-%m-%Y %H:%M:%S\")\n",
    "        df_coin_period = df_coin_period[final_csv_column_order]\n",
    "        df_coin_period.drop_duplicates(subset=['coin','exchange','unix_timestamp'],inplace=True)\n",
    "        df_coin_period.set_index(['coin', 'exchange','unix_timestamp']).to_csv(csv_filename,date_format=datetimeStringformat_to_csv)\n",
    "        #df_new_data.to_csv('Put it into CSV.csv')\n",
    "    print(\"Done\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = ['coin','exchange','unix_timestamp','time','open','high','low','close','volumefrom','volumeto',\n",
    "                 'BBANDS_BANDWIDTH_PERCENT','MACD','MACD_SIGNAL','MACD_TEST','MONEY_FLOW_INDEX','ON_BALANCE_VOLUME','ON_BALANCE_VOLUME_TEST','RSI','RSI_OVER_BOUGHT',\n",
    "                 'RSI_OVER_SOLD','SMA_FAST','SMA_SLOW','SMA_TEST','STOCHRSI_D','STOCHRSI_K','STOCH_OVER_BOUGHT','STOCH_OVER_SOLD','STOCH_PERCENT_D','STOCH_PERCENT_D_MONEY_FLOW_INDEX',\n",
    "                 'STOCH_PERCENT_K','STOCH_PERCENT_K_MONEY_FLOW_INDEX']\n",
    "\n",
    "update_csv_to_latest('1day',datetimeStringformat_to_csv,columns_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = ['coin','exchange','unix_timestamp','time','open','high','low','close','volumefrom','volumeto',\n",
    "                 'BBANDS_BANDWIDTH_PERCENT','MACD','MACD_SIGNAL','MACD_TEST','MONEY_FLOW_INDEX','ON_BALANCE_VOLUME','ON_BALANCE_VOLUME_TEST','RSI','RSI_OVER_BOUGHT',\n",
    "                 'RSI_OVER_SOLD','SMA_FAST','SMA_SLOW','SMA_TEST','STOCHRSI_D','STOCHRSI_K','STOCH_OVER_BOUGHT','STOCH_OVER_SOLD','STOCH_PERCENT_D','STOCH_PERCENT_D_MONEY_FLOW_INDEX',\n",
    "                 'STOCH_PERCENT_K','STOCH_PERCENT_K_MONEY_FLOW_INDEX']\n",
    "update_csv_to_latest('1dayCryptopia',datetimeStringformat_to_csv,columns_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = ['coin','exchange','unix_timestamp','time','open','high','low','close','volumefrom','volumeto',\n",
    "                 'UPPER_BOLLINGER_BAND_VALUE','MIDDLE_BOLLINGER_BAND_VALUE','LOWER_BOLLINGER_BAND_VALUE']\n",
    "update_csv_to_latest('1daybtcbitfinex',datetimeStringformat_to_csv,columns_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_order = ['coin','exchange','unix_timestamp','time','open','high','low','close','volumefrom','volumeto',\n",
    "                 'UPPER_BOLLINGER_BAND_VALUE','MIDDLE_BOLLINGER_BAND_VALUE','LOWER_BOLLINGER_BAND_VALUE']\n",
    "update_csv_to_latest('1hourbtcbitfinex',datetimeStringformat_to_csv,columns_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_list_btc = ['unix_timestamp','UPPER_BOLLINGER_BAND_VALUE','MIDDLE_BOLLINGER_BAND_VALUE','LOWER_BOLLINGER_BAND_VALUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_indicator_BTC(csv_filename,periods,timeframe,datetimeformat_string):\n",
    "    \"\"\" Update the given csv_file with new column values for corr rows \"\"\"\n",
    "    df_csv = pd.read_csv(csv_filename, index_col=None,dayfirst=True)\n",
    "    #df_csv.time = df_csv.time.apply(lambda t: datetime.datetime.strptime(t, '%d-%m-%Y %H:%M:%S'))\n",
    "    \n",
    "    for indicator in indicator_list_btc:\n",
    "        if indicator not in df_csv.columns and indicator not in df_csv.index:\n",
    "            df_csv[indicator] = np.nan\n",
    "    #df_csv.unix_timestamp =  df_csv.time.apply(lambda t: time.mktime(datetime.datetime.strptime(str(t) ,'%d-%m-%Y %H:%M').timetuple()))\n",
    "    df_csv = df_csv.set_index(['coin', 'exchange','unix_timestamp'])\n",
    "    data = list(df_csv.index.get_level_values(0).unique())\n",
    "    i=0\n",
    "    j=0\n",
    "    for coin_name in data:\n",
    "        coin_df = df_csv[df_csv.index.get_level_values(0)==coin_name] \n",
    "        #print(coin_df)\n",
    "        coin_df = coin_df.reset_index()\n",
    "        coin_df = coin_df.sort_values(by=['exchange','unix_timestamp']).set_index(['coin', 'exchange','unix_timestamp'])\n",
    "        #print(coin_df)\n",
    "        df_groupby = coin_df.groupby(['exchange'], group_keys=False)\n",
    "        for key, item in df_groupby:\n",
    "            req_data = df_groupby.get_group(key)\n",
    "            req_data2 = req_data.iloc[-periods:]\n",
    "\n",
    "            start_date = req_data2.index.get_level_values(2)[0]\n",
    "            end_date = req_data2.index.get_level_values(2)[req_data2.shape[0]-1]\n",
    "            req_data2 = req_data[(req_data.index.get_level_values(2) >= start_date) & (req_data.index.get_level_values(2) <= end_date)]\n",
    "            #print(req_data2)\n",
    "            np_volumeto = np.array(req_data2.volumeto.values,dtype='f8')\n",
    "            if len(np_volumeto)<20:\n",
    "                j = j+1\n",
    "                print(coin_name,j,\" Not Updated\")\n",
    "                continue\n",
    "            req_data2['UPPER_BOLLINGER_BAND_VALUE'] = pyti.bollinger_bands.upper_bollinger_band(req_data2.close.values,20)\n",
    "            req_data2['MIDDLE_BOLLINGER_BAND_VALUE'] = pyti.bollinger_bands.middle_bollinger_band(req_data2.close.values,20)\n",
    "            req_data2['LOWER_BOLLINGER_BAND_VALUE'] = pyti.bollinger_bands.lower_bollinger_band(req_data2.close.values,20)\n",
    "            df_csv.update(req_data2)\n",
    "            i = i+1\n",
    "            print(coin_name,i)\n",
    "            #print(df_csv.query('coin == @coin_name').tail(1))\n",
    "            #sys.exit(\"Testing\")\n",
    "    df_csv.to_csv(csv_filename,date_format=datetimeformat_string)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator_BTC('D:\\crypto_analysis-master-master\\crypto_analysis-master-master\\data\\BTC_Bitfinex_day_full_1day.csv',250,'1day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator_BTC('D:\\crypto_analysis-master-master\\crypto_analysis-master-master\\data\\BTC_Bitfinex_day_full_3days.csv',250,'3day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator_BTC('D:\\crypto_analysis-master-master\\crypto_analysis-master-master\\data\\BTC_Bitfinex_day_full_7days.csv',250,'7day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator_BTC('D:\\crypto_analysis-master-master\\crypto_analysis-master-master\\data\\BTC_Bitfinex_day_full_14days.csv',250,'14day',datetimeStringformat_to_csv)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator_BTC('D:\\crypto_analysis-master-master\\crypto_analysis-master-master\\data\\BTC_Bitfinex_hour_full_1hour.csv',250,'1hour',datetimeStringformat_to_csv)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator_BTC('D:\\crypto_analysis-master-master\\crypto_analysis-master-master\\data\\BTC_Bitfinex_hour_full_4hours.csv',250,'4hour',datetimeStringformat_to_csv)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator_BTC('D:\\crypto_analysis-master-master\\crypto_analysis-master-master\\data\\BTC_Bitfinex_hour_full_6hours.csv',250,'6hour',datetimeStringformat_to_csv)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator_BTC('D:\\crypto_analysis-master-master\\crypto_analysis-master-master\\data\\BTC_Bitfinex_hour_full_12hours.csv',250,'12hour',datetimeStringformat_to_csv)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = BackgroundScheduler()\n",
    "scheduler.start()\n",
    "columns_order = ['coin','exchange','unix_timestamp','time','open','high','low','close','volumefrom','volumeto',\n",
    "                 'UPPER_BOLLINGER_BAND_VALUE','MIDDLE_BOLLINGER_BAND_VALUE','LOWER_BOLLINGER_BAND_VALUE']\n",
    "scheduler.add_job(update_csv_to_latest, \"cron\", ['1daybtcbitfinex',datetimeStringformat_to_csv,columns_order], minute='*/1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.shutdown(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeCSVDateTimeFormat(csv_filename):\n",
    "    df_csv = pd.read_csv(csv_filename, index_col=None,dayfirst=True,infer_datetime_format =True)\n",
    "    #df_csv.unix_timestamp =  df_csv.time.apply(lambda t: time.mktime(datetime.datetime.strptime(str(t) ,'%d-%m-%Y %H:%M').timetuple()))\n",
    "    df_csv['time'] = pd.to_datetime(df_csv.unix_timestamp,unit=\"s\",utc=True)\n",
    "    df_csv.set_index(['coin', 'exchange','unix_timestamp']).to_csv(csv_filename,date_format=datetimeStringformat_to_csv)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeCSVDateTimeFormat('all_coins_day_full_1day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeCSVDateTimeFormat('all_coins_day_full_1day_Cryptopia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeCSVDateTimeFormat('BTC_Bitfinex_day_full_1day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeCSVDateTimeFormat('BTC_Bitfinex_hour_full_1hour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( type(datetime.datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
